<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>南方周末 - 刘未鹏</title><link>http://dhunews.sinaapp.com</link><description>南方周末 - 刘未鹏</description><atom:link href="http://dhunews.sinaapp.com/feed" rel="self"></atom:link><language>zh-CN</language><lastBuildDate>Wed, 10 Oct 2012 20:44:39 -0000</lastBuildDate><ttl>240</ttl><item><title>南方周末 - 【专栏】数学之美番外篇：平凡而又神奇的贝叶斯方法（12）</title><link>http://localhost:8080/redirect?url=http%3A%2F%2Fwww.infzm.com%2Fcontent%2F81698</link><description>&lt;div class="fullContent"&gt;
&lt;div style="display:none;"&gt;&lt;/div&gt;
&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;section&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%95%B0%E5%AD%A6/"&gt;数学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/"&gt;贝叶斯方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%8E%A8%E7%90%86/"&gt;推理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span&gt;&lt;/span&gt;&lt;a href="http://www.infzm.com/content/81698#print" onclick="window.print()"&gt;打印&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',-1)"&gt;小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;字体&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',1)"&gt;大&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;div&gt;&lt;/div&gt;

&lt;section&gt;
&lt;p&gt;&lt;strong&gt;隐马可夫模型（HMM）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;吴军在数学之美系列里面介绍的&lt;a href="http://en.wikipedia.org/wiki/Hidden_Markov_model"&gt;隐马可夫模型&lt;/a&gt;（HMM）就是一个简单的层级贝叶斯模型：&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="244" orig_width="304" src="http://images.infzm.com/medias/2012/1010/61049.png@660x440"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;隐马可夫模型&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;那么怎么根据接收到的信息来推测说话者想表达的意思呢？我们可以利用叫做“隐含马尔可夫模型”（Hidden Markov  Model）来解决这些问题。以语音识别为例，当我们观测到语音信号o1,o2,o3时，我们要根据这组信号推测出发送的句子s1,s2,s3。显然，我们应该在所有可能的句子中找最有可能性的一个。用数学语言来描述，就是在已知o1,o2,o3,…的情况下，求使得条件概率P(s1,s2,s3,…|o1,o2,o3….)达到最大值的那个句子 s1,s2,s3,…&lt;/p&gt;
&lt;p&gt;吴军的文章中这里省掉没说的是，s1, s2, s3, ..这个句子的生成概率同时又取决于一组参数，这组参数决定了s1, s2, s3, ..这个马可夫链的先验生成概率。如果我们将这组参数记为λ，我们实际上要求的是：P(S|O, λ)（其中O表示o1,o2,o3,..，S表示s1,s2,s3,..）&lt;/p&gt;
&lt;p&gt;当然，上面的概率不容易直接求出，于是我们可以间接地计算它。利用贝叶斯公式并且省掉一个常数项，可以把上述公式等价变换成&lt;/p&gt;
&lt;p&gt;P(o1,o2,o3,…|s1,s2,s3….) * P(s1,s2,s3,…)&lt;/p&gt;
&lt;p&gt;其中&lt;/p&gt;
&lt;p&gt;P(o1,o2,o3,…|s1,s2,s3….)表示某句话s1,s2,s3…被读成o1,o2,o3,…的可能性，而P(s1,s2,s3,…)表示字串s1,s2,s3,…本身能够成为一个合乎情理的句子的可能性，所以这个公式的意义是用发送信号为s1,s2,s3…这个数列的可能性乘以s1,s2,s3..本身可以一个句子的可能性，得出概率。&lt;/p&gt;
&lt;p&gt;这里，s1,s2,s3…本身可以一个句子的可能性其实就取决于参数λ，也就是语言模型。所以简而言之就是发出的语音信号取决于背后实际想发出的句子，而背后实际想发出的句子本身的独立先验概率又取决于语言模型。&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="246" orig_width="200" src="http://images.infzm.com/medias/2012/0105/50266.jpeg@400x600"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;作者：刘未鹏 出版：电子工业出版社 &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;贝叶斯网络&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;吴军已经对贝叶斯网络作了科普，请直接跳转到&lt;a href="http://googlechinablog.com/2007/01/bayesian-networks.html"&gt;这里&lt;/a&gt;。更详细的理论参考所有机器学习的书上都有。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参考资料&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一堆机器学习，一堆概率统计，一堆Google，和一堆Wikipedia条目，一堆paper。&lt;/p&gt;
&lt;p&gt;部分书籍参考&lt;a href="http://blog.csdn.net/pongba/archive/2008/09/11/2915005.aspx"&gt;《机器学习与人工智能资源导引》&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;（待续；此文的修订版已收录《暗时间》一书，由电子工业出版社2011年8月出版。作者于2009年7月获得南京大学计算机系硕士学位，现在微软亚洲研究院创新工程中心从事软件研发工程师工作。）&lt;/span&gt;&lt;/p&gt; &lt;/section&gt;
&lt;div&gt;&lt;div&gt;&lt;span&gt;上一页&lt;/span&gt;&lt;span title="当前页面"&gt;1&lt;/span&gt;&lt;span&gt;下一页&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div&gt;
								点击阅读&lt;a href="http://www.infzm.com/contents/%E5%88%98%E6%9C%AA%E9%B9%8F%E4%B8%93%E6%A0%8F" target="_blank"&gt;
								刘未鹏专栏&lt;/a&gt;更多内容
							&lt;/div&gt;
&lt;div style="text-align:right"&gt;
&lt;em&gt;网络编辑:&lt;/em&gt;
&lt;em&gt;谢小跳&lt;/em&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Wed, 10 Oct 2012 20:44:39 -0000</pubDate><guid>http://www.infzm.com/content/81698</guid><category>南方周末 - 刘未鹏</category></item><item><title>南方周末 - 【专栏】数学之美番外篇：平凡而又神奇的贝叶斯方法（11）</title><link>http://localhost:8080/redirect?url=http%3A%2F%2Fwww.infzm.com%2Fcontent%2F81596</link><description>&lt;div class="fullContent"&gt;
&lt;div style="display:none;"&gt;&lt;/div&gt;
&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;section&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%95%B0%E5%AD%A6/"&gt;数学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/"&gt;贝叶斯方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%8E%A8%E7%90%86/"&gt;推理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span&gt;&lt;/span&gt;&lt;a href="http://www.infzm.com/content/81596#print" onclick="window.print()"&gt;打印&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',-1)"&gt;小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;字体&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',1)"&gt;大&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;div&gt;&lt;/div&gt;

&lt;section&gt;
&lt;p&gt;&lt;strong&gt;为什么朴素贝叶斯方法令人诧异地好——一个理论解释&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;朴素贝叶斯方法的条件独立假设看上去很傻很天真，为什么结果却很好很强大呢？就拿一个句子来说，我们怎么能鲁莽地声称其中任意一个单词出现的概率只受到它前面的3个或4个单词的影响呢？别说3个，有时候一个单词的概率受到上一句话的影响都是绝对可能的。那么为什么这个假设在实际中的表现却不比决策树差呢？有人对此提出了一个理论解释，并且建立了什么时候朴素贝叶斯的效果能够等价于非朴素贝叶斯的充要条件，这个解释的核心就是：有些独立假设在各个分类之间的分布都是均匀的所以对于似然的相对大小不产生影响；即便不是如此，也有很大的可能性各个独立假设所产生的消极影响或积极影响互相抵消，最终导致结果受到的影响不大。具体的数学公式请参考&lt;a href="http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf"&gt;这篇paper&lt;/a&gt;。&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="246" orig_width="200" src="http://images.infzm.com/medias/2012/0105/50266.jpeg@400x600"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;作者：刘未鹏 出版：电子工业出版社&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;层级贝叶斯模型&lt;/strong&gt;&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="262" orig_width="226" src="http://images.infzm.com/medias/2012/0928/60914.png@400x600"/&gt;&lt;span style="color:#c0c0c0;"&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Hierarchical_Bayes_model"&gt;层级贝叶斯模型&lt;/a&gt;是现代贝叶斯方法的标志性建筑之一。前面讲的贝叶斯，都是在同一个事物层次上的各个因素之间进行统计推理，然而层次贝叶斯模型在哲学上更深入了一层，将这些因素背后的因素（原因的原因，原因的原因，以此类推）囊括进来。一个教科书例子是：如果你手头有N枚硬币，它们是同一个工厂铸出来的，你把每一枚硬币掷出一个结果，然后基于这N个结果对这N个硬币的θ（出现正面的比例）进行推理。如果根据最大似然，每个硬币的θ不是1就是0（这个前面提到过的），然而我们又知道每个硬币的p(θ)是有一个先验概率的，也许是一个beta分布。也就是说，每个硬币的实际投掷结果Xi服从以θ为中心的正态分布，而θ又服从另一个以Ψ为中心的beta分布。层层因果关系就体现出来了。进而Ψ还可能依赖于因果链上更上层的因素，以此类推。&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;（待续；此文的修订版已收录《暗时间》一书，由电子工业出版社2011年8月出版。作者于2009年7月获得南京大学计算机系硕士学位，现在微软亚洲研究院创新工程中心从事软件研发工程师工作。）&lt;/span&gt;&lt;/p&gt; &lt;/section&gt;
&lt;div&gt;&lt;div&gt;&lt;span&gt;上一页&lt;/span&gt;&lt;span title="当前页面"&gt;1&lt;/span&gt;&lt;span&gt;下一页&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div&gt;
								点击阅读&lt;a href="http://www.infzm.com/contents/%E5%88%98%E6%9C%AA%E9%B9%8F%E4%B8%93%E6%A0%8F" target="_blank"&gt;
								刘未鹏专栏&lt;/a&gt;更多内容
							&lt;/div&gt;
&lt;div style="text-align:right"&gt;
&lt;em&gt;网络编辑:&lt;/em&gt;
&lt;em&gt;谢小跳&lt;/em&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Fri, 28 Sep 2012 20:44:39 -0000</pubDate><guid>http://www.infzm.com/content/81596</guid><category>南方周末 - 刘未鹏</category></item><item><title>南方周末 - 【专栏】数学之美番外篇：平凡而又神奇的贝叶斯方法（10）</title><link>http://localhost:8080/redirect?url=http%3A%2F%2Fwww.infzm.com%2Fcontent%2F81595</link><description>&lt;div class="fullContent"&gt;
&lt;div style="display:none;"&gt;&lt;/div&gt;
&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;section&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%95%B0%E5%AD%A6/"&gt;数学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/"&gt;贝叶斯方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%8E%A8%E7%90%86/"&gt;推理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span&gt;&lt;/span&gt;&lt;a href="http://www.infzm.com/content/81595#print" onclick="window.print()"&gt;打印&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',-1)"&gt;小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;字体&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',1)"&gt;大&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;div&gt;&lt;/div&gt;

&lt;section&gt;
&lt;p&gt;&lt;strong&gt;朴素贝叶斯方法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;朴素贝叶斯方法是一个很特别的方法，所以值得介绍一下。我们用朴素贝叶斯在垃圾邮件过滤中的应用来举例说明。&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="246" orig_width="200" src="http://images.infzm.com/medias/2012/0105/50266.jpeg@400x600"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;作者：刘未鹏 出版：电子工业出版社&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;贝叶斯垃圾邮件过滤器&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;问题是什么？问题是，给定一封邮件，判定它是否属于垃圾邮件。按照先例，我们还是用D来表示这封邮件，注意D由N个单词组成。我们用h+来表示垃圾邮件，h-表示正常邮件。问题可以形式化地描述为求：&lt;/p&gt;
&lt;p&gt;P(h+|D) = P(h+) * P(D|h+) / P(D)&lt;/p&gt;
&lt;p&gt;P(h-|D) = P(h-) * P(D|h-) / P(D)&lt;/p&gt;
&lt;p&gt;其中P(h+)和P(h-)这两个先验概率都是很容易求出来的，只需要计算一个邮件库里面垃圾邮件和正常邮件的比例就行了。然而P(D|h+)却不容易求，因为D里面含有N个单词d1, d2, d3, .. ，所以P(D|h+)= P(d1,d2,..,dn|h+)。我们又一次遇到了数据稀疏性，为什么这么说呢？P(d1,d2,..,dn|h+)就是说在垃圾邮件当中出现跟我们目前这封邮件一模一样的一封邮件的概率是多大！开玩笑，每封邮件都是不同的，世界上有无穷多封邮件。瞧，这就是数据稀疏性，因为可以肯定地说，你收集的训练数据库不管里面含了多少封邮件，也不可能找出一封跟目前这封一模一样的。结果呢？我们又该如何来计算P(d1,d2,..,dn|h+)呢？&lt;/p&gt;
&lt;p&gt;我们将P(d1,d2,..,dn|h+)扩展为：P(d1|h+) * P(d2|d1, h+) * P(d3|d2,d1, h+) * ..。熟悉这个式子吗？这里我们会使用一个更激进的假设，我们假设di 与 di-1是完全条件无关的，于是式子就简化为 P(d1|h+) * P(d2|h+) *  P(d3|h+) * .. 。这个就是所谓的&lt;a href="http://en.wikipedia.org/wiki/Conditional_independence"&gt;条件独立假设&lt;/a&gt;，也正是朴素贝叶斯方法的朴素之处。而计算P(d1|h+) * P(d2|h+) * P(d3|h+) * ..就太简单了，只要统计di这个单词在垃圾邮件中出现的频率即可。关于贝叶斯垃圾邮件过滤更多的内容可以参考&lt;a href="http://en.wikipedia.org/wiki/Bayesian_spam_filtering"&gt;这个条目&lt;/a&gt;，注意其中提到的其他资料。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一点注记&lt;/strong&gt;：这里，为什么有这个数据稀疏问题，还是因为统计学习方法工作在浅层面，世界上的单词就算不再变多也是非常之多的，单词之间组成的句子也是变化多端，更不用说一篇文章了，文章数目则是无穷的，所以在这个层面作统计，肯定要被数据稀疏性困扰。我们要注意，虽然句子和文章的数目是无限的，然而就拿邮件来说，如果我们只关心邮件中句子的语义（进而更高抽象层面的“意图”（语义，意图如何可计算地定义出来是一个人工智能问题），在这个层面上可能性便大大缩减了，我们关心的抽象层面越高，可能性越小。单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。神经科学的发现也表明大脑的皮层大致有一种层级结构，对应着越来越抽象的各个层面，至于如何具体实现一个可放在计算机内的大脑皮层，仍然是一个未解决问题，以上只是一个原则（principle）上的认识，只有当computational的cortex模型被建立起来了之后才可能将其放入电脑。&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;（待续；此文的修订版已收录《暗时间》一书，由电子工业出版社2011年8月出版。作者于2009年7月获得南京大学计算机系硕士学位，现在微软亚洲研究院创新工程中心从事软件研发工程师工作。）&lt;/span&gt;&lt;/p&gt; &lt;/section&gt;
&lt;div&gt;&lt;div&gt;&lt;span&gt;上一页&lt;/span&gt;&lt;span title="当前页面"&gt;1&lt;/span&gt;&lt;span&gt;下一页&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div&gt;
								点击阅读&lt;a href="http://www.infzm.com/contents/%E5%88%98%E6%9C%AA%E9%B9%8F%E4%B8%93%E6%A0%8F" target="_blank"&gt;
								刘未鹏专栏&lt;/a&gt;更多内容
							&lt;/div&gt;
&lt;div style="text-align:right"&gt;
&lt;em&gt;网络编辑:&lt;/em&gt;
&lt;em&gt;谢小跳&lt;/em&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Fri, 28 Sep 2012 20:44:39 -0000</pubDate><guid>http://www.infzm.com/content/81595</guid><category>南方周末 - 刘未鹏</category></item><item><title>南方周末 - 【专栏】数学之美番外篇：平凡而又神奇的贝叶斯方法（9）</title><link>http://localhost:8080/redirect?url=http%3A%2F%2Fwww.infzm.com%2Fcontent%2F81594</link><description>&lt;div class="fullContent"&gt;
&lt;div style="display:none;"&gt;&lt;/div&gt;
&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;section&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%95%B0%E5%AD%A6/"&gt;数学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/"&gt;贝叶斯方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%8E%A8%E7%90%86/"&gt;推理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span&gt;&lt;/span&gt;&lt;a href="http://www.infzm.com/content/81594#print" onclick="window.print()"&gt;打印&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',-1)"&gt;小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;字体&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',1)"&gt;大&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;div&gt;&lt;/div&gt;

&lt;section&gt;
&lt;p&gt;&lt;strong&gt;最大似然与最小二乘&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;学过线性代数的大概都知道经典的最小二乘方法来做线性回归。问题描述是：给定平面上N个点，（这里不妨假设我们想用一条直线来拟合这些点——&lt;a href="http://en.wikipedia.org/wiki/Regression_analysis"&gt;回归&lt;/a&gt;可以看作是&lt;a href="http://en.wikipedia.org/wiki/Curve_fitting"&gt;拟合&lt;/a&gt;的特例，即允许误差的拟合），找出一条最佳描述了这些点的直线。&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="232" orig_width="184" src="http://images.infzm.com/medias/2012/0928/60913.png@400x600"/&gt;&lt;span style="color:#c0c0c0;"&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;一个接踵而来的问题就是，我们如何定义最佳？我们设每个点的坐标为(Xi, Yi)。如果直线为y = f(x)。那么(Xi, Yi)跟直线对这个点的“预测”：(Xi, f(Xi))就相差了一个ΔYi = |Yi – f(Xi)|。最小二乘就是说寻找直线使得(ΔY1)^2 +  (ΔY2)^2 + ..（即误差的平方和）最小，至于为什么是误差的平方和而不是误差的绝对值和，统计学上也没有什么好的解释。然而贝叶斯方法却能对此提供一个完美的解释。&lt;/p&gt;
&lt;p&gt;我们假设直线对于坐标Xi给出的预测f(Xi)是最靠谱的预测，所有纵坐标偏离f(Xi)的那些数据点都含有噪音，是噪音使得它们偏离了完美的一条直线，一个合理的假设就是偏离路线越远的概率越小，具体小多少，可以用一个正态分布曲线来模拟，这个分布曲线以直线对Xi给出的预测f(Xi)为中心，实际纵坐标为Yi的点(Xi,Yi)发生的概率就正比于EXP[-(ΔYi)^2]。（EXP(..)代表以常数e为底的多少次方）。&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="246" orig_width="200" src="http://images.infzm.com/medias/2012/0105/50266.jpeg@400x600"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;作者：刘未鹏 出版：电子工业出版社 &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;现在我们回到问题的贝叶斯方面，我们要想最大化的后验概率是：&lt;/p&gt;
&lt;p&gt;P(h|D) ∝ P(h) * P(D|h)&lt;/p&gt;
&lt;p&gt;又见贝叶斯！这里h就是指一条特定的直线，D就是指这N个数据点。我们需要寻找一条直线h使得P(h) * P(D|h)最大。很显然，P(h)这个先验概率是均匀的，因为哪条直线也不比另一条更优越。所以我们只需要看P(D|h)这一项，这一项是指这条直线生成这些数据点的概率，刚才说过了，生成数据点(Xi, Yi)的概率为EXP[-(ΔYi)^2]乘以一个常数。而 P(D|h) = P(d1|h) * P(d2|h) * ..  即假设各个数据点是独立生成的，所以可以把每个概率乘起来。于是生成N个数据点的概率为EXP[-(ΔY1)^2] * EXP[-(ΔY2)^2] *  EXP[-(ΔY3)^2] * .. = EXP&lt;strong&gt;{-[(ΔY1)^2 + (ΔY2)^2 + (ΔY3)^2 + ..&lt;/strong&gt;]}最大化这个概率就是要最小化(ΔY1)^2 + (ΔY2)^2 + (ΔY3)^2 + .. 。熟悉这个式子吗？&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;（待续；此文的修订版已收录《暗时间》一书，由电子工业出版社2011年8月出版。作者于2009年7月获得南京大学计算机系硕士学位，现在微软亚洲研究院创新工程中心从事软件研发工程师工作。）&lt;/span&gt;&lt;/p&gt; &lt;/section&gt;
&lt;div&gt;&lt;div&gt;&lt;span&gt;上一页&lt;/span&gt;&lt;span title="当前页面"&gt;1&lt;/span&gt;&lt;span&gt;下一页&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div&gt;
								点击阅读&lt;a href="http://www.infzm.com/contents/%E5%88%98%E6%9C%AA%E9%B9%8F%E4%B8%93%E6%A0%8F" target="_blank"&gt;
								刘未鹏专栏&lt;/a&gt;更多内容
							&lt;/div&gt;
&lt;div style="text-align:right"&gt;
&lt;em&gt;网络编辑:&lt;/em&gt;
&lt;em&gt;谢小跳&lt;/em&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Fri, 28 Sep 2012 20:44:39 -0000</pubDate><guid>http://www.infzm.com/content/81594</guid><category>南方周末 - 刘未鹏</category></item><item><title>南方周末 - 【专栏】数学之美番外篇：平凡而又神奇的贝叶斯方法（8）</title><link>http://localhost:8080/redirect?url=http%3A%2F%2Fwww.infzm.com%2Fcontent%2F81407</link><description>&lt;div class="fullContent"&gt;
&lt;div style="display:none;"&gt;&lt;/div&gt;
&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;section&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%95%B0%E5%AD%A6/"&gt;数学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/"&gt;贝叶斯方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%8E%A8%E7%90%86/"&gt;推理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span&gt;&lt;/span&gt;&lt;a href="http://www.infzm.com/content/81407#print" onclick="window.print()"&gt;打印&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',-1)"&gt;小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;字体&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',1)"&gt;大&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;div&gt;&lt;/div&gt;

&lt;section&gt;
&lt;p&gt;&lt;strong&gt;贝叶斯图像识别，Analysis by Synthesis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;贝叶斯方法是一个非常general的推理框架。其核心理念可以描述成：Analysis by Synthesis（通过合成来分析）。06年的认知科学新进展上有一篇paper就是讲用贝叶斯推理来解释视觉识别的，一图胜千言，下图就是摘自这篇paper：&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="216" orig_width="604" src="http://images.infzm.com/medias/2012/0926/60780.jpeg@660x440"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;贝叶斯图像识别&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;首先是视觉系统提取图形的边角特征，然后使用这些特征自底向上地激活高层的抽象概念（比如是E还是F还是等号），然后使用一个自顶向下的验证来比较到底哪个概念最佳地解释了观察到的图像。&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="246" orig_width="200" src="http://images.infzm.com/medias/2012/0105/50266.jpeg@400x600"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;作者：刘未鹏 出版：电子工业出版社&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;EM&lt;/strong&gt;&lt;strong&gt;算法与基于模型的聚类&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Data_clustering"&gt;聚类&lt;/a&gt;是一种&lt;a href="http://en.wikipedia.org/wiki/Unsupervised_learning"&gt;无指导的机器学习&lt;/a&gt;问题，问题描述：给你一堆数据点，让你将它们最靠谱地分成一堆一堆的。聚类算法很多，不同的算法适应于不同的问题，这里仅介绍一个基于模型的聚类，该聚类算法对数据点的假设是，这些数据点分别是围绕K个核心的K个正态分布源所随机生成的，使用Han JiaWei的《Data Ming： Concepts and  Techniques》中的图：&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="324" orig_width="532" src="http://images.infzm.com/medias/2012/0926/60781.jpeg@660x440"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;聚类&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;图中有两个正态分布核心，生成了大致两堆点。我们的聚类算法就是需要根据给出来的那些点，算出这两个正态分布的核心在什么位置，以及分布的参数是多少。这很明显又是一个贝叶斯问题，但这次不同的是，答案是连续的且有无穷多种可能性，更糟的是，只有当我们知道了哪些点属于同一个正态分布圈的时候才能够对这个分布的参数作出靠谱的预测，现在两堆点混在一块我们又不知道哪些点属于第一个正态分布，哪些属于第二个。反过来，只有当我们对分布的参数作出了靠谱的预测时候，才能知道到底哪些点属于第一个分布，那些点属于第二个分布。这就成了一个先有鸡还是先有蛋的问题了。为了解决这个循环依赖，总有一方要先打破僵局，说，不管了，我先随便整一个值出来，看你怎么变，然后我再根据你的变化调整我的变化，然后如此迭代着不断互相推导，最终收敛到一个解。这就是EM算法。&lt;/p&gt;
&lt;p&gt;EM的意思是“Expectation-Maximazation”，在这个聚类问题里面，我们是先随便猜一下这两个正态分布的参数：如核心在什么地方，方差是多少。然后计算出每个数据点更可能属于第一个还是第二个正态分布圈，这个是属于Expectation一步。有了每个数据点的归属，我们就可以根据属于第一个分布的数据点来重新评估第一个分布的参数（从蛋再回到鸡），这个是Maximazation。如此往复，直到参数基本不再发生变化为止。这个迭代收敛过程中的贝叶斯方法在第二步，根据数据点求分布的参数上面。&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;（待续；此文的修订版已收录《暗时间》一书，由电子工业出版社2011年8月出版。作者于2009年7月获得南京大学计算机系硕士学位，现在微软亚洲研究院创新工程中心从事软件研发工程师工作。）&lt;/span&gt;&lt;/p&gt; &lt;/section&gt;
&lt;div&gt;&lt;div&gt;&lt;span&gt;上一页&lt;/span&gt;&lt;span title="当前页面"&gt;1&lt;/span&gt;&lt;span&gt;下一页&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div&gt;
								点击阅读&lt;a href="http://www.infzm.com/contents/%E5%88%98%E6%9C%AA%E9%B9%8F%E4%B8%93%E6%A0%8F" target="_blank"&gt;
								刘未鹏专栏&lt;/a&gt;更多内容
							&lt;/div&gt;
&lt;div style="text-align:right"&gt;
&lt;em&gt;网络编辑:&lt;/em&gt;
&lt;em&gt;谢小跳&lt;/em&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Wed, 26 Sep 2012 20:44:39 -0000</pubDate><guid>http://www.infzm.com/content/81407</guid><category>南方周末 - 刘未鹏</category></item><item><title>南方周末 - 【专栏】数学之美番外篇：平凡而又神奇的贝叶斯方法（7）</title><link>http://localhost:8080/redirect?url=http%3A%2F%2Fwww.infzm.com%2Fcontent%2F81331</link><description>&lt;div class="fullContent"&gt;
&lt;div style="display:none;"&gt;&lt;/div&gt;
&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;section&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%95%B0%E5%AD%A6/"&gt;数学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/"&gt;贝叶斯方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%8E%A8%E7%90%86/"&gt;推理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span&gt;&lt;/span&gt;&lt;a href="http://www.infzm.com/content/81331#print" onclick="window.print()"&gt;打印&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',-1)"&gt;小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;字体&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',1)"&gt;大&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;div&gt;&lt;/div&gt;

&lt;section&gt;
&lt;p&gt;&lt;strong&gt;统计机器翻译&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;统计机器翻译因为其简单，自动（无需手动添加规则），迅速成为了机器翻译的事实标准。而统计机器翻译的核心算法也是使用的贝叶斯方法。&lt;/p&gt;
&lt;p&gt;问题是什么？统计机器翻译的问题可以描述为：给定一个句子e，它的可能的外文翻译f中哪个是最靠谱的。即我们需要计算：P(f|e)。一旦出现条件概率贝叶斯总是挺身而出：&lt;/p&gt;
&lt;p&gt;P(f|e) ∝ P(f) * P(e|f)&lt;/p&gt;
&lt;p&gt;这个式子的右端很容易解释：那些先验概率较高，并且更可能生成句子e的外文句子f将会胜出。我们只需简单统计（结合上面提到的N-Gram语言模型）就可以统计任意一个外文句子f的出现概率。然而P(e|f)却不是那么好求的，给定一个候选的外文局子f，它生成（或对应）句子e的概率是多大呢？我们需要定义什么叫“对应”，这里需要用到一个分词对齐的平行语料库，有兴趣的可以参考《Foundations of Statistical  Natural Language Processing》第13章，这里摘选其中的一个例子：假设e为：John loves Mary。我们需要考察的首选f是：Jean aime Marie（法文）。我们需要求出P(e|f)是多大，为此我们考虑e和f有多少种对齐的可能性，如：&lt;/p&gt;
&lt;p&gt;John (Jean) loves (aime) Marie (Mary)&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" orig_height="246" orig_width="200" src="http://images.infzm.com/medias/2012/0105/50266.jpeg@300x450"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;作者：刘未鹏 出版：电子工业出版社&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;就是其中的一种（最靠谱的）对齐，为什么要对齐，是因为一旦对齐了之后，就可以容易地计算在这个对齐之下的P(e|f)是多大，只需计算：&lt;/p&gt;
&lt;p&gt;P(John|Jean) * P(loves|aime) * P(Marie|Mary)&lt;/p&gt;
&lt;p&gt;即可。&lt;/p&gt;
&lt;p&gt;然后我们遍历所有的对齐方式，并将每种对齐方式之下的翻译概率∑求和。便可以获得整个的P(e|f)是多大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一点注记&lt;/strong&gt;：还是那个问题：难道我们人类真的是用这种方式进行翻译的？highly unlikely。这种计算复杂性非常高的东西连三位数乘法都搞不定的我们才不会笨到去使用呢。根据认知神经科学的认识，很可能我们是先从句子到语义（一个逐层往上（bottom-up）抽象的folding过程），然后从语义根据另一门语言的语法展开为另一门语言（一个逐层往下（top-down）的具体化unfolding过程）。如何可计算地实现这个过程，目前仍然是个难题。（我们看到很多地方都有bottom-up/top-down这样一个对称的过程，实际上有人猜测这正是生物神经网络原则上的运作方式，对视觉神经系统的研究尤其证明了这一点，Hawkins在《On  Intelligence》里面提出了一种&lt;a href="http://en.wikipedia.org/wiki/Hierarchical_Temporal_Memory"&gt;HTM&lt;/a&gt;（Hierarchical Temporal Memory）模型正是使用了这个原则。）&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;（待续；此文的修订版已收录《暗时间》一书，由电子工业出版社2011年8月出版。作者于2009年7月获得南京大学计算机系硕士学位，现在微软亚洲研究院创新工程中心从事软件研发工程师工作。）&lt;/span&gt;&lt;/p&gt; &lt;/section&gt;
&lt;div&gt;&lt;div&gt;&lt;span&gt;上一页&lt;/span&gt;&lt;span title="当前页面"&gt;1&lt;/span&gt;&lt;span&gt;下一页&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div&gt;
								点击阅读&lt;a href="http://www.infzm.com/contents/%E5%88%98%E6%9C%AA%E9%B9%8F%E4%B8%93%E6%A0%8F" target="_blank"&gt;
								刘未鹏专栏&lt;/a&gt;更多内容
							&lt;/div&gt;
&lt;div style="text-align:right"&gt;
&lt;em&gt;网络编辑:&lt;/em&gt;
&lt;em&gt;谢小跳&lt;/em&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Mon, 24 Sep 2012 20:44:39 -0000</pubDate><guid>http://www.infzm.com/content/81331</guid><category>南方周末 - 刘未鹏</category></item><item><title>南方周末 - 【专栏】数学之美番外篇：平凡而又神奇的贝叶斯方法（6）</title><link>http://localhost:8080/redirect?url=http%3A%2F%2Fwww.infzm.com%2Fcontent%2F81005</link><description>&lt;div class="fullContent"&gt;
&lt;div style="display:none;"&gt;&lt;/div&gt;
&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;section&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%95%B0%E5%AD%A6/"&gt;数学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/"&gt;贝叶斯方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%8E%A8%E7%90%86/"&gt;推理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span&gt;&lt;/span&gt;&lt;a href="http://www.infzm.com/content/81005#print" onclick="window.print()"&gt;打印&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',-1)"&gt;小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;字体&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',1)"&gt;大&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;div&gt;&lt;/div&gt;

&lt;section&gt;
&lt;p&gt;&lt;strong&gt;无处不在的贝叶斯&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;以下我们再举一些实际例子来说明贝叶斯方法被运用的普遍性，这里主要集中在机器学习方面，因为我不是学经济的，否则还可以找到一堆经济学的例子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中文分词&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;贝叶斯是机器学习的核心方法之一。比如中文分词领域就用到了贝叶斯。Google研究员吴军在《数学之美》系列中就有一篇是介绍中文分词的，这里只介绍一下核心的思想，不做赘述，详细请参考吴军的文章（&lt;a href="http://www.googlechinablog.com/2006/04/blog-post_10.html"&gt;这里&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;分词问题的描述为：给定一个句子（字串），如：&lt;/p&gt;
&lt;p&gt;南京市长江大桥&lt;/p&gt;
&lt;p&gt;如何对这个句子进行分词（词串）才是最靠谱的。例如：&lt;/p&gt;
&lt;p&gt;1.南京市/长江大桥&lt;/p&gt;
&lt;p&gt;2.南京/市长/江大桥&lt;/p&gt;
&lt;p&gt;这两个分词，到底哪个更靠谱呢？&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" height="246" src="http://images.infzm.com/medias/2012/0105/50266.jpeg" width="200"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;作者：刘未鹏 出版：电子工业出版社&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;我们用贝叶斯公式来形式化地描述这个问题，令X为字串（句子），Y为词串（一种特定的分词假设）。我们就是需要寻找使得P(Y|X)最大的Y，使用一次贝叶斯可得：&lt;/p&gt;
&lt;p&gt;P(Y|X) ∝ P(Y)*P(X|Y)&lt;/p&gt;
&lt;p&gt;用自然语言来说就是这种分词方式（词串）的可能性乘以这个词串生成我们的句子的可能性。我们进一步容易看到：可以近似地将P(X|Y)看作是恒等于1的，因为任意假想的一种分词方式之下生成我们的句子总是精准地生成的（只需把分词之间的分界符号扔掉即可）。于是，我们就变成了去最大化P(Y)，也就是寻找一种分词使得这个词串（句子）的概率最大化。而如何计算一个词串：&lt;/p&gt;
&lt;p&gt;W1,W2,W3,W4...&lt;/p&gt;
&lt;p&gt;的可能性呢？我们知道，根据&lt;a href="http://en.wikipedia.org/wiki/Joint_probability"&gt;联合概率&lt;/a&gt;的公式展开：P(W1, W2, W3, W4 ..) = P(W1) * P(W2|W1) * P(W3|W2, W1) * P(W4|W1,W2,W3) * ..于是我们可以通过一系列的条件概率（右式）的乘积来求整个联合概率。然而不幸的是随着条件数目的增加（P(Wn|Wn-1,Wn-2,..,W1)的条件有n-1个），&lt;a href="http://en.wikipedia.org/wiki/Curse_of_dimensionality"&gt;数据稀疏问题&lt;/a&gt;也会越来越严重，即便语料库再大也无法统计出一个靠谱的P(Wn|Wn-1,Wn-2,..,W1)来。为了缓解这个问题，计算机科学家们一如既往地使用了“天真”假设：我们假设句子中一个词的出现概率只依赖于它前面的有限的k个词（k一般不超过3，如果只依赖于前面的一个词，就是2元&lt;a href="http://en.wikipedia.org/wiki/N-gram"&gt;语言模型&lt;/a&gt;（2-gram），同理有3-gram、4-gram等），这个就是所谓的“有限地平线”假设。虽然这个假设很傻很天真，但结果却表明它的结果往往是很好很强大的，后面要提到的朴素贝叶斯方法使用的假设跟这个精神上是完全一致的，我们会解释为什么像这样一个天真的假设能够得到强大的结果。目前我们只要知道，有了这个假设，刚才那个乘积就可以改写成：P(W1) * P(W2|W1) * P(W3|W2) * P(W4|W3) ...（假设每个词只依赖于它前面的一个词）。而统计P(W2|W1)就不再受到数据稀疏问题的困扰了。对于我们上面提到的例子“南京市长江大桥”，如果按照自左到右的贪婪方法分词的话，结果就成了“南京市长/江大桥”。但如果按照贝叶斯分词的话（假设使用3-gram），由于“南京市长”和“江大桥”在语料库中一起出现的频率为0，这个整句的概率便会被判定为0。从而使得“南京市/长江大桥”这一分词方式胜出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一点注记&lt;/strong&gt;：有人可能会疑惑，难道我们人类也是基于这些天真的假设来进行推理的？不是的。事实上，统计机器学习方法所统计的东西往往处于相当表层（shallow）的层面，在这个层面机器学习只能看到一些非常表面的现象，有一点科学研究的理念的人都知道：越是往表层去，世界就越是繁复多变。从机器学习的角度来说，特征（feature）就越多，成百上千维度都是可能的。特征一多，好了，&lt;a href="http://en.wikipedia.org/wiki/Curse_of_dimensionality"&gt;高维诅咒&lt;/a&gt;就产生了，数据就稀疏得要命，不够用了。而我们人类的观察水平显然比机器学习的观察水平要更深入一些，为了避免数据稀疏我们不断地发明各种装置（最典型就是显微镜），来帮助我们直接深入到更深层的事物层面去观察更本质的联系，而不是在浅层对表面现象作统计归纳。举一个简单的例子，通过对大规模语料库的统计，机器学习可能会发现这样一个规律：所有的“他”都是不会穿bra的，所有的“她”则都是穿的。然而，作为一个男人，却完全无需进行任何统计学习，因为深层的规律就决定了我们根本不会去穿bra。至于机器学习能不能完成后者（像人类那样的）这个推理，则是人工智能领域的经典问题。至少在那之前，&lt;a href="http://www.yeeyan.com/articles/view/sylviaangel/9995"&gt;声称统计学习方法能够终结科学研究&lt;/a&gt;（&lt;a href="http://www.wired.com/science/discoveries/magazine/16-07/pb_theory"&gt;原文&lt;/a&gt;）的说法&lt;a href="http://scienceblogs.com/goodmath/2008/07/petabyte_scale_dataanalysis_an.php"&gt;是纯粹外行人说的话&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;（待续；此文的修订版已收录《暗时间》一书，由电子工业出版社2011年8月出版。作者于2009年7月获得南京大学计算机系硕士学位，现在微软亚洲研究院创新工程中心从事软件研发工程师工作。）&lt;/span&gt;&lt;/p&gt; &lt;/section&gt;
&lt;div&gt;&lt;div&gt;&lt;span&gt;上一页&lt;/span&gt;&lt;span title="当前页面"&gt;1&lt;/span&gt;&lt;span&gt;下一页&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div&gt;
								点击阅读&lt;a href="http://www.infzm.com/contents/%E5%88%98%E6%9C%AA%E9%B9%8F%E4%B8%93%E6%A0%8F" target="_blank"&gt;
								刘未鹏专栏&lt;/a&gt;更多内容
							&lt;/div&gt;
&lt;div style="text-align:right"&gt;
&lt;em&gt;网络编辑:&lt;/em&gt;
&lt;em&gt;谢小跳&lt;/em&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Mon, 17 Sep 2012 20:44:39 -0000</pubDate><guid>http://www.infzm.com/content/81005</guid><category>南方周末 - 刘未鹏</category></item><item><title>南方周末 - 【专栏】数学之美番外篇：平凡而又神奇的贝叶斯方法（5）</title><link>http://localhost:8080/redirect?url=http%3A%2F%2Fwww.infzm.com%2Fcontent%2F81004</link><description>&lt;div class="fullContent"&gt;
&lt;div style="display:none;"&gt;&lt;/div&gt;
&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;section&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%95%B0%E5%AD%A6/"&gt;数学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/"&gt;贝叶斯方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%8E%A8%E7%90%86/"&gt;推理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span&gt;&lt;/span&gt;&lt;a href="http://www.infzm.com/content/81004#print" onclick="window.print()"&gt;打印&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',-1)"&gt;小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;字体&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',1)"&gt;大&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;div&gt;&lt;/div&gt;

&lt;section&gt;
&lt;p&gt;&lt;b&gt;最小描述长度原则&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;贝叶斯模型比较理论与信息论有一个有趣的关联：&lt;/p&gt;
&lt;p&gt;P(h | D) ∝ P(h) * P(D | h)&lt;/p&gt;
&lt;p&gt;两边求对数，将右式的乘积变成相加：&lt;/p&gt;
&lt;p&gt;ln P(h | D) ∝ ln P(h) + ln P(D | h)&lt;/p&gt;
&lt;p&gt;显然，最大化P(h | D) 也就是最大化 ln P(h | D)。而ln P(h) + ln P(D | h)则可以解释为模型（或者称“假设”、“猜测”）h的编码长度加上在该模型下数据D的编码长度。使这个和最小的模型就是最佳模型。&lt;/p&gt;
&lt;p&gt;而究竟如何定义一个模型的编码长度，以及数据在模型下的编码长度则是一个问题。（更多可参考Mitchell的《Machine Learning》[1]的6.6节，或Mackay的28.3节）&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" height="246" src="http://images.infzm.com/medias/2012/0105/50266.jpeg" width="200"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;作者：刘未鹏 出版：电子工业出版社 &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;b&gt;最优贝叶斯推理&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;所谓的推理，分为两个过程，第一步是对观测数据建立一个模型。第二步则是使用这个模型来推测未知现象发生的概率。我们前面都是讲的对于观测数据给出最靠谱的那个模型。然而很多时候，虽然某个模型是所有模型里面最靠谱的，但是别的模型也并不是一点机会都没有。譬如第一个模型在观测数据下的概率是0.5。第二个模型是0.4，第三个是0.1。如果我们只想知道对于观测数据哪个模型最可能，那么只要取第一个就行了，故事到此结束。然而很多时候我们建立模型是为了推测未知的事情的发生概率，这个时候，三个模型对未知的事情发生的概率都会有自己的预测，仅仅因为某一个模型概率稍大一点就只听他一个人的就太不民主了。所谓的最优贝叶斯推理就是将三个模型对于未知数据的预测结论加权平均起来（权值就是模型相应的概率）。显然，这个推理是理论上的制高点，无法再优了，因为它已经把所有可能性都考虑进去了。&lt;/p&gt;
&lt;p&gt;只不过实际上我们是基本不会使用这个框架的，因为计算模型可能非常费时间，二来模型空间可能是连续的，即有无穷多个模型（这个时候需要计算模型的概率分布）。结果还是非常费时间。所以这个被看作是一个理论基准。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;注释：&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;[1]中译名《机器学习》。&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;（待续；此文的修订版已收录《暗时间》一书，由电子工业出版社2011年8月出版。作者于2009年7月获得南京大学计算机系硕士学位，现在微软亚洲研究院创新工程中心从事软件研发工程师工作。）&lt;/span&gt;&lt;/p&gt; &lt;/section&gt;
&lt;div&gt;&lt;div&gt;&lt;span&gt;上一页&lt;/span&gt;&lt;span title="当前页面"&gt;1&lt;/span&gt;&lt;span&gt;下一页&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div&gt;
								点击阅读&lt;a href="http://www.infzm.com/contents/%E5%88%98%E6%9C%AA%E9%B9%8F%E4%B8%93%E6%A0%8F" target="_blank"&gt;
								刘未鹏专栏&lt;/a&gt;更多内容
							&lt;/div&gt;
&lt;div style="text-align:right"&gt;
&lt;em&gt;网络编辑:&lt;/em&gt;
&lt;em&gt;谢小跳&lt;/em&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Mon, 17 Sep 2012 20:44:39 -0000</pubDate><guid>http://www.infzm.com/content/81004</guid><category>南方周末 - 刘未鹏</category></item><item><title>南方周末 - 【专栏】数学之美番外篇：平凡而又神奇的贝叶斯方法（4）</title><link>http://localhost:8080/redirect?url=http%3A%2F%2Fwww.infzm.com%2Fcontent%2F80731</link><description>&lt;div class="fullContent"&gt;
&lt;div style="display:none;"&gt;&lt;/div&gt;
&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;section&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%95%B0%E5%AD%A6/"&gt;数学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/"&gt;贝叶斯方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%8E%A8%E7%90%86/"&gt;推理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span&gt;&lt;/span&gt;&lt;a href="http://www.infzm.com/content/80731#print" onclick="window.print()"&gt;打印&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',-1)"&gt;小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;字体&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',1)"&gt;大&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;div&gt;&lt;/div&gt;

&lt;section&gt;
&lt;p&gt;&lt;strong&gt;模型比较理论（Model Comparasion）与贝叶斯奥卡姆剃刀（Bayesian  Occam’s Razor）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;实际上，模型比较就是去比较哪个模型（猜测）更可能隐藏在观察数据的背后。其基本思想前面已经用拼写纠正的例子来说明了。我们对用户实际想输入的单词的猜测就是模型，用户输错的单词就是观测数据。我们通过：&lt;/p&gt;
&lt;p&gt;P(h|D)∝P(h)*P(D|h)&lt;/p&gt;
&lt;p&gt;来比较哪个模型最为靠谱。前面提到，光靠P(D|h)（即“似然”）是不够的，有时候还需要引入P(h)这个先验概率。奥卡姆剃刀就是说P(h)较大的模型有较大的优势，而最大似然则是说最符合观测数据的（即P(D|h)最大的）最有优势。整个模型比较就是这两方力量的拉锯。我们不妨再举一个简单的例子来说明这一精神：你随便找枚硬币，掷一下，观察一下结果。好，你观察到的结果要么是“正”，要么是“反”（不，不是少林足球那枚硬币:P），不妨假设你观察到的是“正”。现在你要去根据这个观测数据推断这枚硬币掷出“正”的概率是多大。根据最大似然估计的精神，我们应该猜测这枚硬币掷出“正”的概率是1，因为这个才是能最大化P(D|h)的那个猜测。然而每个人都会大摇其头——很显然，你随机摸出一枚硬币这枚硬币居然没有反面的概率是“不存在的”，我们对一枚随机硬币是否一枚有偏硬币，偏了多少，是有着一个先验的认识的，这个认识就是绝大多数硬币都是基本公平的，偏得越多的硬币越少见（可以用一个&lt;a href="http://en.wikipedia.org/wiki/Beta_distribution"&gt;beta  分布&lt;/a&gt;来表达这一先验概率）。将这个先验正态分布p(θ)（其中θ表示硬币掷出正面的比例，小写的p代表这是&lt;a href="http://en.wikipedia.org/wiki/Probability_density_function"&gt;概率密度函数&lt;/a&gt;）结合到我们的问题中，我们便不是去最大化P(D|h)，而是去最大化P(D|θ)*p(θ)，显然θ=1是不行的，因为P(θ=1)为0，导致整个乘积也为0。实际上，只要对这个式子求一个导数就可以得到最值点。&lt;/p&gt;
&lt;p&gt;以上说的是当我们知道先验概率P(h)的时候，光用最大似然是不靠谱的，因为最大似然的猜测可能先验概率非常小。然而，有些时候，我们对于先验概率一无所知，只能假设每种猜测的先验概率是均等的，这个时候就只有用最大似然了。实际上，统计学家和贝叶斯学家有一个有趣的争论，统计学家说：我们让数据自己说话。言下之意就是要摒弃先验概率。而贝叶斯支持者则说：数据会有各种各样的偏差，而一个靠谱的先验概率则可以对这些随机噪音做到健壮。事实证明贝叶斯派胜利了，胜利的关键在于所谓先验概率其实也是经验统计的结果，譬如为什么我们会认为绝大多数硬币是基本公平的？为什么我们认为大多数人的肥胖适中？为什么我们认为肤色是种族相关的，而体重则与种族无关？先验概率里面的“先验”并不是指先于一切经验，而是仅指先于我们“当前”给出的观测数据而已，在硬币的例子中先验指的只是先于我们知道投掷的结果这个经验，而并非“先天”。&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" height="246" src="http://images.infzm.com/medias/2012/0105/50266.jpeg" width="200"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;作者：刘未鹏 出版：电子工业出版社&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;然而，话说回来，有时候我们必须得承认，就算是基于以往的经验，我们手头的“先验”概率还是均匀分布，这个时候就必须依赖用最大似然，我们用前面留下的一个自然语言二义性问题来说明这一点：&lt;/p&gt;
&lt;p&gt;The girl saw the boy with a telescope.&lt;/p&gt;
&lt;p&gt;到底是The girl saw-with-a-telescope the boy这一语法结构，还是The girl saw  the-boy-with-a-telescope呢？两种语法结构的常见程度都差不多（你可能会觉得后一种语法结构的常见程度较低，这是事后偏见，你只需想想The  girl saw the boy with a book就知道了。当然，实际上从大规模语料统计结果来看后一种语法结构的确稍稍不常见一丁点，但是绝对不足以解释我们对第一种结构的强烈倾向）。那么到底为什么呢？&lt;/p&gt;
&lt;p&gt;我们不妨先来看看MacKay在书中举的一个漂亮的例子：&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" height="371" src="http://images.infzm.com/medias/2012/0911/59985.jpeg" width="384"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;font-size:1.0em"&gt;树后面到底有多少个箱子？&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;图中有多少个箱子？特别地，那棵书后面是一个箱子？还是两个箱子？还是三个箱子？你可能会觉得树后面肯定是一个箱子，但为什么不是两个呢？如下图：&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" height="368" src="http://images.infzm.com/medias/2012/0911/59986.jpeg" width="262"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;font-size:1.0em"&gt;两种可能的解释&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;很简单，你会说：要是真的有两个箱子那才怪了，怎么就那么巧这两个箱子刚刚好颜色相同，高度相同呢？&lt;/p&gt;
&lt;p&gt;用概率论的语言来说，你刚才的话就翻译为：猜测h不成立，因为P(D|h)太小（太巧合）了。我们的直觉是：巧合（小概率）事件不会发生。所以当一个猜测（假设）使得我们的观测结果成为小概率事件的时候，我们就说“才怪呢，哪能那么巧呢？！”&lt;/p&gt;
&lt;p&gt;现在我们可以回到那个自然语言二义性的例子，并给出一个完美的解释了：如果语法结构是The girl saw the-boy-with-a-telecope的话，怎么那个男孩偏偏手里拿的就是望远镜——一个可以被用来saw-with的东东呢？这也忒小概率了吧。他咋就不会拿本书呢？拿什么都好。怎么偏偏就拿了望远镜？所以唯一的解释是，这个“巧合”背后肯定有它的必然性，这个必然性就是，如果我们将语法结构解释为The girl saw-with-a-telescope the boy的话，就跟数据完美吻合了——既然那个女孩是用某个东西去看这个男孩的，那么这个东西是一个望远镜就完全可以解释了（不再是小概率事件了）。&lt;/p&gt;
&lt;p&gt;自然语言二义性很常见，譬如这句话就有二义性：&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;参见《决策与判断》以及《&lt;/span&gt;&lt;a href="http://www.douban.com/subject/3199621/"&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;Rationality for  Mortals&lt;/span&gt;&lt;/a&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;》第12章：小孩也可以解决贝叶斯问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;到底是参见这两本书的第12章，还是仅仅是第二本书的第12章呢？如果是这两本书的第12章那就是咄咄怪事了，怎么恰好两本书都有第12章，都是讲同一个问题，更诡异的是，标题还相同呢？&lt;/p&gt;
&lt;p&gt;注意，以上做的是似然估计（即只看P(D|h)的大小），不含先验概率。通过这两个例子，尤其是那个树后面的箱子的例子我们可以看到，似然估计里面也蕴含着奥卡姆剃刀：树后面的箱子数目越多，这个模型就越复杂。单个箱子的模型是最简单的。似然估计选择了更简单的模型。&lt;/p&gt;
&lt;p&gt;这个就是所谓的&lt;strong&gt;贝叶斯奥卡姆剃刀（Bayesian Occam’s  Razor）&lt;/strong&gt;，因为这个剃刀工作在贝叶斯公式的似然（P(D|h)）上，而不是模型本身（P(h)）的先验概率上，后者是传统的奥卡姆剃刀。关于贝叶斯奥卡姆剃刀我们再来看一个前面说到的曲线拟合的例子：如果平面上有N个点，近似构成一条直线，但绝不精确地位于一条直线上。这时我们既可以用直线来拟合（模型1），也可以用二阶多项式（模型2）拟合，也可以用三阶多项式（模型3）……，特别地，用N-1阶多项式便能够保证肯定能完美通过N个数据点。那么，这些可能的模型之中到底哪个是最靠谱的呢？前面提到，一个衡量的依据是奥卡姆剃刀：越是高阶的多项式越是繁复和不常见。然而，我们其实并不需要依赖于这个先验的奥卡姆剃刀，因为有人可能会争辩说：你怎么就能说越高阶的多项式越不常见呢？我偏偏觉得所有阶多项式都是等可能的。好吧，既然如此那我们不妨就扔掉P(h)项，看看P(D|h)能告诉我们什么。我们注意到越是高阶的多项式，它的轨迹弯曲程度越是大，到了八九阶简直就是直上直下，于是我们不仅要问：一个比如说八阶多项式在平面上随机生成的一堆N个点偏偏恰好近似构成一条直线的概率（即P(D|h)）有多大？太小太小了。反之，如果背后的模型是一条直线，那么根据该模型生成一堆近似构成直线的点的概率就大得多了。这就是贝叶斯奥卡姆剃刀。&lt;/p&gt;
&lt;p&gt;这里只是提供一个关于贝叶斯奥卡姆剃刀的科普，强调直观解释，更多理论公式请参考MacKay的著作《Information Theory :  Inference and Learning Algorithms》第28章。&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;（待续；此文的修订版已收录《暗时间》一书，由电子工业出版社2011年8月出版。作者于2009年7月获得南京大学计算机系硕士学位，现在微软亚洲研究院创新工程中心从事软件研发工程师工作。）&lt;/span&gt;&lt;/p&gt; &lt;/section&gt;
&lt;div&gt;&lt;div&gt;&lt;span&gt;上一页&lt;/span&gt;&lt;span title="当前页面"&gt;1&lt;/span&gt;&lt;span&gt;下一页&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div&gt;
								点击阅读&lt;a href="http://www.infzm.com/contents/%E5%88%98%E6%9C%AA%E9%B9%8F%E4%B8%93%E6%A0%8F" target="_blank"&gt;
								刘未鹏专栏&lt;/a&gt;更多内容
							&lt;/div&gt;
&lt;div style="text-align:right"&gt;
&lt;em&gt;网络编辑:&lt;/em&gt;
&lt;em&gt;谢小跳&lt;/em&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Tue, 11 Sep 2012 20:44:39 -0000</pubDate><guid>http://www.infzm.com/content/80731</guid><category>南方周末 - 刘未鹏</category></item><item><title>南方周末 - 【专栏】数学之美番外篇：平凡而又神奇的贝叶斯方法（3）</title><link>http://localhost:8080/redirect?url=http%3A%2F%2Fwww.infzm.com%2Fcontent%2F80696</link><description>&lt;div class="fullContent"&gt;
&lt;div style="display:none;"&gt;&lt;/div&gt;
&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;section&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%95%B0%E5%AD%A6/"&gt;数学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/"&gt;贝叶斯方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tags.infzm.com/tags/tagsearch/tags/%E6%8E%A8%E7%90%86/"&gt;推理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span&gt;&lt;/span&gt;&lt;a href="http://www.infzm.com/content/80696#print" onclick="window.print()"&gt;打印&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',-1)"&gt;小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;字体&lt;/li&gt;
&lt;li&gt;&lt;a href="javascript:void(0)" onclick="conSize('articleContent',1)"&gt;大&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;div&gt;&lt;/div&gt;

&lt;section&gt;
&lt;p&gt;&lt;strong&gt;模型比较与奥卡姆剃刀&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt;&lt;strong&gt;再访拼写纠正&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;介绍了贝叶斯拼写纠正之后，接下来的一个自然而然的问题就来了：“&lt;strong&gt;为什么？&lt;/strong&gt;”为什么要用贝叶斯公式？为什么贝叶斯公式在这里可以用？我们可以很容易地领会为什么贝叶斯公式用在前面介绍的那个男生女生长裤裙子的问题里是正确的。但为什么这里？&lt;/p&gt;
&lt;p&gt;为了回答这个问题，一个常见的思路就是想想：&lt;strong&gt;非得这样吗？&lt;/strong&gt;因为如果你想到了另一种做法并且证明了它也是靠谱的，那么将它与现在这个一比较，也许就能得出很有价值的信息。那么对于拼写纠错问题你能想到其他方案吗？&lt;/p&gt;
&lt;p&gt;不管怎样，一个最常见的替代方案就是，选择离thew的&lt;a href="http://en.wikipedia.org/wiki/Edit_distance"&gt;编辑距离&lt;/a&gt;最近的。然而the和thaw离thew的编辑距离都是1。这可咋办？你说，不慌，那还是好办。我们就看到底哪个更可能被错打为thew就是了。我们注意到字母e和字母w在键盘上离得很紧，无名指一抽筋就不小心多打出一个w来，the就变成thew了。而另一方面thaw被错打成thew的可能性就相对小一点，因为e和a离得较远而且使用的指头相差一个指头（一个是中指一个是小指，不像e和w使用的指头靠在一块——神经科学的证据表明紧邻的身体设施之间容易串位）。OK，很好，因为你现在已经是在用最大似然方法了，或者直白一点，你就是在计算那个使得P(D|h)最大的h。&lt;/p&gt;
&lt;div&gt;
&lt;p style="text-align:center;"&gt;&lt;img alt="" height="246" src="http://images.infzm.com/medias/2012/0105/50266.jpeg" width="200"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;color:#666;line-height:1.4em;padding:0 4em;"&gt;作者：刘未鹏 出版：电子工业出版社&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;而贝叶斯方法计算的是什么？是P(h)*P(D|h)。多出来了一个P(h)。我们刚才说了，这个多出来的P(h)是特定猜测的先验概率。为什么要掺和进一个先验概率？刚才说的那个最大似然不是挺好么？很雄辩地指出了the是更靠谱的猜测。有什么问题呢？既然这样，我们就从给最大似然找茬开始吧——我们假设两者的似然程度是一样或非常相近，这样不就难以区分哪个猜测更靠谱了吗？比如用户输入tlp，那到底是top还是tip？（这个例子不怎么好，因为top和tip的词频可能仍然是接近的，但一时想不到好的英文单词的例子，我们不妨就假设top比tip常见许多吧，这个假设并不影响问题的本质。）这个时候，当最大似然不能作出决定性的判断时，先验概率就可以插手进来给出指示——“既然你无法决定，那么我告诉你，一般来说top出现的程度要高许多，所以更可能他想打的是top”）。&lt;/p&gt;
&lt;p&gt;以上只是最大似然的一个问题，即并不能提供决策的全部信息。&lt;/p&gt;
&lt;p&gt;最大似然还有另一个问题：即便一个猜测与数据非常符合，也并不代表这个猜测就是更好的猜测，因为这个猜测本身的可能性也许就非常低。比如MacKay在《Information Theory : Inference and Learning Algorithms》里面就举了一个很好的例子：-1 3 7 11你说是等差数列更有可能呢？还是 -X^3 / 11 + 9/11*X^2 + 23/11 每项把前项作为X带入后计算得到的数列？此外曲线拟合也是，平面上N个点总是可以用N-1阶多项式来完全拟合，当N个点近似但不精确共线的时候，用N-1阶多项式来拟合能够精确通过每一个点，然而用直线来做拟合/线性回归的时候却会使得某些点不能位于直线上。你说到底哪个好呢？多项式？还是直线？一般地说肯定是越低阶的多项式越靠谱（当然前提是也不能忽视“似然”P(D|h)，明摆着一个多项式分布您愣是去拿直线拟合也是不靠谱的，这就是为什么要把它们两者乘起来考虑。），原因之一就是低阶多项式更常见，先验概率（P(h)）较大（原因之二则隐藏在P(D|h)里面），这就是为什么我们要用&lt;a href="http://en.wikipedia.org/wiki/Spline_interpolation"&gt;样条&lt;/a&gt;来插值，而不是直接搞一个N-1阶多项式来通过任意N个点的原因。&lt;/p&gt;
&lt;p&gt;以上分析当中隐含的哲学是，观测数据总是会有各种各样的误差，比如观测误差（比如你观测的时候一个MM经过你一不留神，手一抖就是一个误差出现了），所以如果过分去寻求能够完美解释观测数据的模型，就会落入所谓的数据&lt;a href="http://en.wikipedia.org/wiki/Overfitting"&gt;过配（overfitting）&lt;/a&gt;的境地，一个过配的模型试图连误差（噪音）都去解释（而实际上噪音又是不需要解释的），显然就过犹不及了。所以P(D|h)大不代表你的h（猜测）就是更好的h。还要看P(h)是怎样的。所谓&lt;a href="http://en.wikipedia.org/wiki/Occam%27s_razor"&gt;奥卡姆剃刀&lt;/a&gt;精神就是说：如果两个理论具有相似的解释力度，那么优先选择那个更简单的（往往也正是更平凡的，更少繁复的，更常见的）。&lt;/p&gt;
&lt;p&gt;过分匹配的另一个原因在于当观测的结果并不是因为误差而显得“不精确”而是因为真实世界中对数据的结果产生贡献的因素太多太多，跟噪音不同，这些偏差是一些另外的因素集体贡献的结果，不是你的模型所能解释的——噪音那是不需要解释——一个现实的模型往往只提取出几个与结果相关度很高，很重要的因素（cause）。这个时候观察数据会倾向于围绕你的有限模型的预测结果呈&lt;a href="http://en.wikipedia.org/wiki/Normal_Distribution"&gt;正态分布&lt;/a&gt;，于是你实际观察到的结果就是这个正态分布的&lt;a href="http://en.wikipedia.org/wiki/Random_sample"&gt;随机取样&lt;/a&gt;，这个取样很可能受到其余因素的影响偏离你的模型所预测的中心，这个时候便不能贪心不足地试图通过改变模型来“完美”匹配数据，因为那些使结果偏离你的预测的贡献因素不是你这个有限模型里面含有的因素所能概括的，硬要打肿脸充胖子只能导致不实际的模型，举个教科书例子：身高和体重的实际关系近似于一个二阶多项式的关系，但大家都知道并不是只有身高才会对体重产生影响，物理世界影响体重的因素太多太多了，有人身材高大却瘦得跟稻草，有人却是横长竖不长。但不可否认的是总体上来说，那些特殊情况越是特殊就越是稀少，呈围绕最普遍情况（胖瘦适中）的正态分布，这个分布就保证了我们的身高——体重相关模型能够在大多数情况下做出靠谱的预测。但是——刚才说了，特例是存在的，就算不是特例，人有胖瘦，密度也有大小，所以完美符合身高——体重的某个假想的二阶多项式关系的人是不存在的，我们又不是欧几里德几何世界当中的理想多面体，所以，当我们对人群随机抽取了N个样本（数据点）试图对这N个数据点拟合出一个多项式的话就得注意，它肯定得是二阶多项式，我们要做的只是去根据数据点计算出多项式各项的参数（一个典型的方法就是最小二乘）；它肯定不是直线（我们又不是稻草），也不是三阶多项式四阶多项式。如果硬要完美拟合N个点，你可能会整出一个N-1阶多项式来——设想身高和体重的关系是5阶多项式看看？&lt;/p&gt;
&lt;p&gt;&lt;span style="font-family: 楷体,楷体_GB2312,STKaiti;"&gt;（待续；此文的修订版已收录《暗时间》一书，由电子工业出版社2011年8月出版。作者于2009年7月获得南京大学计算机系硕士学位，现在微软亚洲研究院创新工程中心从事软件研发工程师工作。）&lt;/span&gt;&lt;/p&gt; &lt;/section&gt;
&lt;div&gt;&lt;div&gt;&lt;span&gt;上一页&lt;/span&gt;&lt;span title="当前页面"&gt;1&lt;/span&gt;&lt;span&gt;下一页&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div&gt;
								点击阅读&lt;a href="http://www.infzm.com/contents/%E5%88%98%E6%9C%AA%E9%B9%8F%E4%B8%93%E6%A0%8F" target="_blank"&gt;
								刘未鹏专栏&lt;/a&gt;更多内容
							&lt;/div&gt;
&lt;div style="text-align:right"&gt;
&lt;em&gt;网络编辑:&lt;/em&gt;
&lt;em&gt;谢小跳&lt;/em&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Tue, 11 Sep 2012 20:44:39 -0000</pubDate><guid>http://www.infzm.com/content/80696</guid><category>南方周末 - 刘未鹏</category></item></channel></rss>